# LogService Ochestrationa in Kubernetes

| Status        | Proposed   |
:-------------- |:---------- |
| **Authors**   | @aylei     |
| **Freshness** | 2022-06-28 |

## Abstract

This proposal describes how to ochestrate the Pods of a LogService in k8s(Kubernetes).

## Background

LogService is a distributed system which requires specific operation knowledge to manage its lifecycle, including boostrap, upgrading, failover, etc.
While these knowledges are tranditionally held by human operators, coding it to an automated supervisor program achieve more responsive and predictable operations. Such program is so-called "Controller" (a.k.a. "Operator") in k8s.

This proposal is based on the following designs and assumes the reader has read them in advance:

- [MatrixOne Operator Runtime](./runtime.md)

### Glossary

- `Cluster`: a cluster is a group of LogService Pods (might be heterogenous) that functions as a whole;
- `LogSet`: a k8s object that records the desired state and actual state of a LogSrevice cluster;
- `Pod`: an isolation unit of resources and network in k8s. `Nodes` are abstracted away from the applications by k8s so applications can aware nothing but Pod. Though there are lots of difference, treating Pods as virtual machines might help understading this document.
- `Node`: a logical collection of compute resources in k8s, each Node run multiple Pods. `Nodes` in k8s should be distinguished from `nodes` in distributed systems, while in the k8s context each `node` in a distributed system is actually refers to a k8s `Pod`.

## Proposal

### Overview

For each `LogSet` object, the controller will maintain the following resources:

- A `StatefulSet`, which manages the Pods of the `LogSet` cluster. `StatefulSet` provides stable mapping from Pod ID (which is also be used as Pod DNS name) to the persistent state (volume) of that ID, which ensures a Pod with certain DNS name always has same persistent state attached across pod deletions and re-creations;
  - `StatefulSet` also allows assigning multiple persistent volumes to each Pod;
- A k8s `Service` for the discovery port of HA Keeper, which distribute traffic evenly to the discovery port of all Pods in `LogSet`, which is expected to return the routing info of the current HA Keeper leader;
- A list of k8s `ConfigMaps` , which records the configuration of Pods in `LogSet`;

The controller will take care of the cluster [Bootstrap](#bootstrap), [Toplogy Management](#store-topology), [Failover](#store-failover), [Rolling-update](#rolling-update) and basic [Observability](#observability). Detailed descriptions started below.

### Bootstrap

When the controller observed a `LogSet` object, it should determine whether the cluster need bootstrap.
Since the controller is stateless and actual state might be unknown (e.g. no enough actual states are observed due to network partition),
the controller will always persistent the bootstrap decision to ETCD before performing actual bootstrap.

Bootstrap decision includes:

- the initial stores that are selected to run HAKeeper replicas;
- the `ReplicaID` of the initial HAKeeper replicas;

Pursue-do code of bootstrap:

```go
func (c *Controller) Bootstrap(logSet *LogSet) {
    // initial StoreID to initial HA keeper ReplicaID
    var initialHAKeepers map[string]string
    previousDecision, hasBootstrapped := logSet.Annotations["matrixorigin.io/ha-keeper-bootstrap"]
    if !hasBootstrapped {
        // like {"1": "10005", "2": "10006", "3: "10007"}
        initialHAKeepers = SelectInitialHAKeepers(logSet)
        logSet.Annotations["matrixorigin.io/ha-keeper-bootstrap"] = Encode(initialHAKeepers)
        // persist the decision to ETCD, if we crash before update, it is ok to select
        // the initial ha keepers again since no decision is actually dispatched
        c.Update(logSet)
    } else {
        initialHAKeepers = Decode(previousDecision)
    }
    initialHAKeeperConfig := ToConfig(initialHAKeepers)
    // pass initialHAKeeperConfig to all Pods 
}
```

Note that all the Pods will receive the same config that describes which stores should be act as the initial HA Keepers.
For Pods not in the initial list, they simply ignore the config.

An `LogSet` object with HA keeper replication factor greater than log store replicas is invalid and should be rejected on create or update.

### Store Topology

As mentioned earlier, the application does not aware k8s nodes. But HA scheduling requires infrastucure topology information to make decision, e.g. spread replicas of a specific shard to different racks. To bridge the gap, we've support adding labels to each store to provide the topology information.

In k8s environment, the Controller will be responsible for setting store labels for each log store via configuration file or command-lien arguments. Here are some well known label examples:

- `kubernetes.io/hostname: "ip-172-20-114-199.ec2.internal"`
- `topology.kubernetes.io/zone: "us-east-1a"`
- `topology.kubernetes.io/region: "us-east-1"`

In addition to exposing the topology information via store labels, the Controller should also allows fine-grained control over store topology so that users of the Controller can easily make their trade-off between availability and cost-efficiency. For example, spread log stores across available zones improve availability but also introduce additional cross-zone traffic cost.

There are many ways to do this in k8s, but the most reliable one is to use [`topologySpreadConstraint`](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/). Since the constraint policy is complicated, `LogSet` object provide a syntactic sugar for end users:

```go
type LogSet struct {
    // TopologyKeys instruct the controller to spread the log store evenly
    // across the given topology keys.
    TopologyKeys []string
}

func example() {
    // this example means the log set should be evenly spread in each region, and the partition in each region
    // should be further evenly spread in each zone, then host machine (hostname).
    ls := &LogSet{
        TopologyKeys []string{"region", "zone", "hostname"}
    }
}
```

By default, `TopologySpreads` is empty and allows multiple log stores to be placed on a same node for demonstration purpose. `TopologySpreadConstraint` will also be customizable for advanced user (typically mo-cloud) to allow more fine-grained control like uneven spread.

Object validation should check `ToplogyKeys` and `TopologySpreadConstraint` must not be set at the same time to avoid ambiguity.

### Store Failover

When a store process exit, the container runtime will always restart the process. But there are scenarios where restarting does not work, which can be categorized by whether Pod level failover is possible:

1. Able to failover at Pod level: the underlying k8s Node fails and the Pod is using network disk;
2. Unable to failover at Pod level:
    - Node fails and the Pod is using local disk;
    - store fails to restart continuously, e.g. due to data corruption or network partition;

The second category cannot be discovered solely at k8s level since the store process may running but actually not functioning. So the cluster info from HA Keeper will also be considered by the Controller to discover a failure.

The Controller will categorize the failover scenario first and then apply different failover strategy:

- For the first category, the controller will failover the Pod to another Node. The new topology should still satisfy the topology constraint described in [store toplogy](#store-topology). If the topology cannot be satisfies, the new Pod will be created but stuck in Pending state. Node provision or repair will be triggered in such case, either by another controller or a human operator;
- For the second category, the controller will add new Pod to the LogSet, which then register itself as a new LogStore. HA Keeper then is responsible for repair shard replicas as usual. Once the Controller observed all the replicas are repaired via the `GetClusterInfo` API, the failed store is considered obsolete and the compute resources of it can be reclaimed;

It is worth noting that HA Keeper also keeps repairing failed replicas in the first category of scenarios. So replicas on the failed store might be scheduled to a running store first and then be rebalanced to the original store after failover, which cause unnecessary load on running stores.

Thus, the replica failure timeout should be configurable. The controller will configure the timeout longer then the P95 time of a Pod failover to avoid unnecessary replica rescheduling. Longer timeout may increase the failover duration for the second category, this problem can be eliminated by the same strategy described in the [planned migration](#planned-migration) section.

### Rolling-update

There are several scenarios require an rolling-update of the `LogSet` cluster:

- Vertical scaling;
- Upgrade cluster version, which is equivalent to replacing the container image of each Pod;
- Mutate configuration and the mutation cannot be hot reloaded by the application;
- Migrate Pods to other infrastructure, e.g. from general purpose EC2 instances to network optimized EC2 instances or from nodes of a k8s version to nodes of another k8s version;
  - In this section, we assume the persistent volume is a network storage (which can be detached and re-attached to another VM). Migration on local disk requires extra attention and will be discussed later.

To minimize the impact of rolling-update, we want to transfer the leaders of each raft group gracefully instead of killing the process and wait an election timeout to trigger an new leader election. The application is expected to transfer out the leader proactively during graceful shutdown or expose an leader transfer API to the controller for the same purpose.

The native k8s `StatefulSet` always rolling-update Pods in descending order of Pod ID.
Such behavior will cause unnecessary leader transfer: say we have 3 stores with ID [0, 1, 2] and the leader is current in 2, when rolling-update the Pods in descending order, the leader will be transferred out from 2 and be transferred again when we update 0 or 1.

A better strategy is to rolling-update the Pods in ascending order of leader count, specially, we always want to update the Pod that runs the leader of HA keeper last to minimize the leader transfers of HA keeper.

An [opensource extension of k8s `StatefulSet`](https://openkruise.io/docs/user-manuals/advancedstatefulset) is introduced to support such ordering. The controller will set priority of each Pod according to their leader count and instruct the extended statefulset to rolling-update Pods in priority order.

If any Pod is failed during rolling-update, the rolling-update will be paused. [Alert](#observability) and [failover](#failover) will also take place, which are orthogonal to rolling-update and will be discussed in dedicated sections.

### Planned Migration

There are several scenarios require planned migration:

- Scale in a `LogSet` cluster, the log shard replicas on Pods to be reclaimed should be migrated first;
- Migrate Pods to other infrastructure when using local SSD;

This document propose to add a `DeleteStore` API in HA Keeper, which indicates the compute resources of the store is going to be reclaimed.
A successful `DeleteStore` request set the Store to `Terminating` state.
All shard replicas should be migrated out from `Terminating` Store and no new replicas can be scheduled to a `Terminating` Store.

Controller will call `DeleteStore` API during planned migration and poll `GetClusterInfo` API to wait all the replicas are drained from the `Terminating` Store before actually reclaim the compute resources. Cluster configuration change algorithms like joint consensus ensures that we can always tolerant minority failure during migration while we can only tolerant `minority - 1` store failure during failover. This is especially important when replication factor is 3, where `minority - 1` is 0.

Planned migration is a nice to have feature and we can be treated it the same as failover before we add it.

### Observability

The [MO controller framework](./runtime.md) will automatically export the following o11y data:

- Prometheus metrics about the condition of LogSet, e.g. how long does the LogSet take to be ready, how many Pods are ready, etc.;
- Tracing of the controller actions;

The prometheus metrics can be used to calculate alert conditions like "HA Keeper is unavailable", "A shard has a minority failure not being repaired for xxx minutes", etc.
